from frequencyman.text_processing import LangId, TextProcessing

USER_PROVIDED_TOKENIZERS = TextProcessing.get_user_provided_tokenizers()


def test_acceptable_word():

    is_acceptable_word = TextProcessing.get_word_accepter(None)
    is_acceptable_word_zh = TextProcessing.get_word_accepter(LangId("zh"))
    is_acceptable_word_vi = TextProcessing.get_word_accepter(LangId("vi"))

    assert is_acceptable_word("app-le") == True
    assert is_acceptable_word("app---le") == True
    assert is_acceptable_word("won't") == True
    assert is_acceptable_word("iÃ±tÃ«rnÃ¢tiÃ´nÃ lizÃ¦tiÃ¸n") == True

    assert is_acceptable_word("s") == False
    assert is_acceptable_word("s ") == False
    assert is_acceptable_word("s!!!") == False
    assert is_acceptable_word("_s_") == False
    assert is_acceptable_word("1234") == False
    assert is_acceptable_word("1.123") == False
    assert is_acceptable_word("$$$$$$$") == False
    assert is_acceptable_word("___") == False

    assert is_acceptable_word_zh("ä½ ") == True

    assert is_acceptable_word_vi("Ä‘Æ°á»ng") == True


def test_get_plain_text():

    assert TextProcessing.get_plain_text("This a <b>sample</b></b> text with HTML<p>tags</P>and<br>example.") == "This a sample text with HTML tags and example."
    assert TextProcessing.get_plain_text("This a <script> kk </script> example.") == "This a example."
    assert TextProcessing.get_plain_text("This N&amp;M that") == "This N&M that"
    assert TextProcessing.get_plain_text("Non-breaking space: This&nbsp;is&nbsp;a&nbsp;test.") == "Non-breaking space: This is a test."
    assert TextProcessing.get_plain_text("This [sound:azure-57d4ff9d-b02968eb-9f293f42-b1c6426f-2dd6b8a7.mp3]") == "This"
    assert TextProcessing.get_plain_text("Use of [brackets] <b>[inside HTML]</b> tags.") == "Use of [brackets] [inside HTML] tags."


def test_get_word_tokens_from_text_default_tokenizer_en():

    text_with_special_chars = "I can't \"believe\" it's here (wow) 1-character! 'amazinG'"
    assert TextProcessing.get_word_tokens_from_text(text_with_special_chars, LangId("en")) == ["can't", "believe", "it", "here", "wow", "1-character", "amazing"]

    assert TextProcessing.get_word_tokens_from_text("", LangId("en")) == []
    assert TextProcessing.get_word_tokens_from_text("12345", LangId("en")) == []

    assert TextProcessing.get_word_tokens_from_text("Simple sente-nce. EE.UU. Anna's visit. john's. o'clock", LangId("en")) == ['simple', 'sente-nce', 'ee.uu', 'anna', 'visit', 'john', 'o\'clock']
    assert TextProcessing.get_word_tokens_from_text("This is a\ttest.\t", LangId("en")) == ['this', 'is', 'test']
    assert TextProcessing.get_word_tokens_from_text("Hello, world!", LangId("en")) == ['hello', 'world']

    assert TextProcessing.get_word_tokens_from_text("ĞœĞ½Ğµ â€¦ Ğ»ĞµÑ‚.", LangId("en")) == ['Ğ¼Ğ½Ğµ', 'Ğ»ĞµÑ‚']
    assert TextProcessing.get_word_tokens_from_text("ä½ è·Ÿæˆ‘", LangId("en")) == ['ä½ è·Ÿæˆ‘']
    assert TextProcessing.get_word_tokens_from_text("Ø£Ù†Ø§ Ø¨ØµØ¯Ù‘Ù‚Ùƒ ÙˆØ¥Ø­Ù†Ø§ Ø¯Ø§ÙŠÙ…Ø§Ù‹ Ù…Ù†Ù‚Ù„ÙƒÙ…", LangId("en")) == ['Ø£Ù†Ø§', 'Ø¨ØµØ¯Ù‘Ù‚Ùƒ', 'ÙˆØ¥Ø­Ù†Ø§', 'Ø¯Ø§ÙŠÙ…Ø§Ù‹', 'Ù…Ù†Ù‚Ù„ÙƒÙ…']


def test_get_word_tokens_from_text_user_tokenizer_th():
    # no support for tokenization, but Thai script should be accepted and not split
    assert TextProcessing.get_word_tokens_from_text("à¸­à¸²à¸«à¸²à¸£ hello", LangId("th")) == ['à¸­à¸²à¸«à¸²à¸£']

def test_get_word_tokens_from_text_user_tokenizer_lo():
    # no support for tokenization, but Lao script should be accepted and not split
    assert TextProcessing.get_word_tokens_from_text("àº®àº±àº hello à¸­à¸²à¸«à¸²à¸£", LangId("lo")) == ['àº®àº±àº']

def test_get_word_tokens_from_text_user_tokenizer_km():
    # no support for tokenization, but Khmer script should be accepted and not split
    assert TextProcessing.get_word_tokens_from_text("á‡áŸ†ášá¶á”áŸá½áš hello àº®àº±àº", LangId("km")) == ['á‡áŸ†ášá¶á”áŸá½áš']


def test_get_word_tokens_from_text_user_tokenizer_ja():

    if not USER_PROVIDED_TOKENIZERS.lang_has_tokenizer(LangId('ja')):
        print("User tokenizer for JA test skipped!!!")
        return

    for tokenizer in USER_PROVIDED_TOKENIZERS.get_all_tokenizers(LangId('ja')):
        print("User tokenizer for JA tested: "+tokenizer.__name__)
        assert TextProcessing.get_word_tokens_from_text("ã™ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã®ã†ã¡", LangId('ja'), tokenizer) == ['ã™ã‚‚ã‚‚', 'ã‚‚', 'ã‚‚ã‚‚', 'ã‚‚', 'ã‚‚ã‚‚', 'ã®', 'ã†ã¡'], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("ã“ã‚“ã«ã¡ã¯ã€‚ç§ã®åå‰ã¯ã‚·ãƒ£ãƒ³ã§ã™ã€‚", LangId('ja'), tokenizer) == ["ã“ã‚“ã«ã¡ã¯", "ç§", "ã®", "åå‰", "ã¯", "ã‚·ãƒ£ãƒ³", "ã§ã™"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆã§ã™ã€‚", LangId('ja'), tokenizer) == ["ã“ã‚Œ", "ã¯", "ãƒ†ã‚¹ãƒˆ", "ã§ã™"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("ç§ã®åå‰ã¯å¤ªéƒã§ã™ ã€‚", LangId('ja'), tokenizer) == ["ç§", "ã®", "åå‰", "ã¯", "å¤ªéƒ", "ã§ã™"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("ã‚ã®ã‚¤ãƒ«ã‚«ã¯ã¨ã¦ã‚‚ã‹ã‚ã„ã„ï¼", LangId('ja'), tokenizer) == ["ã‚ã®", "ã‚¤ãƒ«ã‚«", "ã¯", "ã¨ã¦ã‚‚", "ã‹ã‚ã„ã„"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("å›ã®è¨€è‘‰ã¯æ·±ã„ã§ã™ã­ã€‚", LangId('ja'), tokenizer) == ["å›", "ã®", "è¨€è‘‰", "ã¯", "æ·±ã„", "ã§ã™", "ã­"], tokenizer.__name__


def test_get_word_tokens_from_text_user_tokenizer_zh():

    if not USER_PROVIDED_TOKENIZERS.lang_has_tokenizer(LangId('zh')):
        print("User tokenizer for ZH test skipped!!!")
        return

    for tokenizer in USER_PROVIDED_TOKENIZERS.get_all_tokenizers(LangId('zh')):
        print("User tokenizer for ZH tested: "+tokenizer.__name__)
        assert TextProcessing.get_word_tokens_from_text("ä½ è·Ÿæˆ‘", LangId('zh'), tokenizer) == ['ä½ ', 'è·Ÿ', 'æˆ‘'], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("hello åƒé¥­äº†å—ğŸ˜Š? ", LangId('zh'), tokenizer) == ["åƒé¥­", "äº†", "å—"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†ã€‚", LangId('zh'), tokenizer) == ['æˆ‘', 'çˆ±', 'è‡ªç„¶è¯­è¨€', 'å¤„ç†'], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text(" æˆ‘ çˆ±è‡ªç„¶è¯­è¨€å¤„ç†ã€‚ ", LangId('zh'), tokenizer) == ['æˆ‘', 'çˆ±', 'è‡ªç„¶è¯­è¨€', 'å¤„ç†'], tokenizer.__name__


def test_create_word_token():

    assert TextProcessing.create_word_token("$$hellO", LangId('en')) == "hello"
    assert TextProcessing.create_word_token("hello!", LangId('en')) == "hello"
    assert TextProcessing.create_word_token("weâ€™ll", LangId('en')) == "we'll"
    assert TextProcessing.create_word_token("IÃ±tÃ«rnÃ¢tiÃ´nÃ lizÃ¦tiÃ¸n", LangId('en')) == "iÃ±tÃ«rnÃ¢tiÃ´nÃ lizÃ¦tiÃ¸n"


def test_calc_word_presence_score():

    token_context = TextProcessing.get_word_tokens_from_text("test", LangId("en"))
    assert TextProcessing.calc_word_presence_score(token_context[0], token_context, 0) == 1.0

    token_context = TextProcessing.get_word_tokens_from_text("test abcd", LangId("en"))
    assert TextProcessing.calc_word_presence_score(token_context[0], token_context, 0) == (0.5+0.5+1)/3

    token_context = TextProcessing.get_word_tokens_from_text("test abcdabcd", LangId("en"))
    assert TextProcessing.calc_word_presence_score(token_context[0], token_context, 0) == (0.5+(1/3)+1)/3

    token_context = TextProcessing.get_word_tokens_from_text("test test abcd", LangId("en"))
    assert TextProcessing.calc_word_presence_score(token_context[0], token_context, 1) == ((2/3)+(2/3)+0.5)/3
