from typing import Sequence
from frequencyman.text_processing import LangId, TextProcessing, WordToken
from frequencyman.tokenizers import get_tokenizer_registry


def test_acceptable_word():

    is_acceptable_word = TextProcessing.get_word_accepter(None)
    is_acceptable_word_zh = TextProcessing.get_word_accepter(LangId("zh"))
    is_acceptable_word_vi = TextProcessing.get_word_accepter(LangId("vi"))

    assert is_acceptable_word("app-le") == True
    assert is_acceptable_word("app---le") == True
    assert is_acceptable_word("won't") == True
    assert is_acceptable_word("i√±t√´rn√¢ti√¥n√†liz√¶ti√∏n") == True

    assert is_acceptable_word("s") == False
    assert is_acceptable_word("s ") == False
    assert is_acceptable_word("s!!!") == False
    assert is_acceptable_word("_s_") == False
    assert is_acceptable_word("1234") == False
    assert is_acceptable_word("1.123") == False
    assert is_acceptable_word("$$$$$$$") == False
    assert is_acceptable_word("___") == False

    assert is_acceptable_word_zh("‰Ω†") == True

    assert is_acceptable_word_vi("ƒë∆∞·ªùng") == True


def test_get_plain_text():

    assert TextProcessing.get_plain_text("This a <b>sample</b></b> text with HTML<p>tags</P>and<br>example.") == "This a sample text with HTML tags and example."
    assert TextProcessing.get_plain_text("This a <script> kk </script> example.") == "This a example."
    assert TextProcessing.get_plain_text("This N&amp;M that") == "This N&M that"
    assert TextProcessing.get_plain_text("Non-breaking space: This&nbsp;is&nbsp;a&nbsp;test.") == "Non-breaking space: This is a test."
    assert TextProcessing.get_plain_text("This [sound:azure-57d4ff9d-b02968eb-9f293f42-b1c6426f-2dd6b8a7.mp3]") == "This"
    assert TextProcessing.get_plain_text("Use of [brackets] <b>[inside HTML]</b> tags.") == "Use of [brackets] [inside HTML] tags."


def test_get_word_tokens_from_text_default_tokenizer_en():

    text_with_special_chars = "I can't \"believe\" it's here (wow) 1-character! 'amazinG'"
    assert TextProcessing.get_word_tokens_from_text(text_with_special_chars, LangId("en")) == ["can't", "believe", "it", "here", "wow", "1-character", "amazing"]

    assert TextProcessing.get_word_tokens_from_text("", LangId("en")) == []
    assert TextProcessing.get_word_tokens_from_text("12345", LangId("en")) == []

    assert TextProcessing.get_word_tokens_from_text("Anna‚Äôs sente-nce. EE.UU. Anna's visit. john's. o'clock", LangId("en")) == ['anna', 'sente-nce', 'ee.uu', 'anna', 'visit', 'john', 'o\'clock']
    assert TextProcessing.get_word_tokens_from_text("This is a\ttest.\t", LangId("en")) == ['this', 'is', 'test']
    assert TextProcessing.get_word_tokens_from_text("Hello, world!", LangId("en")) == ['hello', 'world']

    assert TextProcessing.get_word_tokens_from_text("–ú–Ω–µ ‚Ä¶ –ª–µ—Ç.", LangId("en")) == ['–º–Ω–µ', '–ª–µ—Ç']
    assert TextProcessing.get_word_tokens_from_text("‰Ω†Ë∑üÊàë", LangId("en")) == ['‰Ω†Ë∑üÊàë']
    assert TextProcessing.get_word_tokens_from_text("ÿ£ŸÜÿß ÿ®ÿµÿØŸëŸÇŸÉ Ÿàÿ•ÿ≠ŸÜÿß ÿØÿßŸäŸÖÿßŸã ŸÖŸÜŸÇŸÑŸÉŸÖ", LangId("en")) == ['ÿ£ŸÜÿß', 'ÿ®ÿµÿØŸëŸÇŸÉ', 'Ÿàÿ•ÿ≠ŸÜÿß', 'ÿØÿßŸäŸÖÿßŸã', 'ŸÖŸÜŸÇŸÑŸÉŸÖ']


def test_get_word_tokens_from_text_user_tokenizer_th():
    # no support for tokenization, but Thai script should be accepted and not split
    assert TextProcessing.get_word_tokens_from_text("‡∏≠‡∏≤‡∏´‡∏≤‡∏£ hello", LangId("th")) == ['‡∏≠‡∏≤‡∏´‡∏≤‡∏£']

def test_get_word_tokens_from_text_user_tokenizer_lo():
    # no support for tokenization, but Lao script should be accepted and not split
    assert TextProcessing.get_word_tokens_from_text("‡∫Æ‡∫±‡∫Å hello ‡∏≠‡∏≤‡∏´‡∏≤‡∏£", LangId("lo")) == ['‡∫Æ‡∫±‡∫Å']

def test_get_word_tokens_from_text_user_tokenizer_km():
    # no support for tokenization, but Khmer script should be accepted and not split
    assert TextProcessing.get_word_tokens_from_text("·ûá·üÜ·ûö·û∂·ûî·ûü·ûΩ·ûö hello ‡∫Æ‡∫±‡∫Å", LangId("km")) == ['·ûá·üÜ·ûö·û∂·ûî·ûü·ûΩ·ûö']


def test_get_word_tokens_from_text_user_tokenizer_ja():

    ja_tokenizers = [tokenizer for tokenizer in get_tokenizer_registry() if LangId('ja') in tokenizer.supported_languages()]

    assert(len(ja_tokenizers) >= 3)

    for tokenizer in ja_tokenizers:
        print("User tokenizer for JA tested: "+tokenizer.name())
        assert TextProcessing.get_word_tokens_from_text("„Åô„ÇÇ„ÇÇ„ÇÇ„ÇÇ„ÇÇ„ÇÇ„ÇÇ„ÇÇ„ÅÆ„ÅÜ„Å°", LangId('ja'), tokenizer) == ['„Åô„ÇÇ„ÇÇ', '„ÇÇ', '„ÇÇ„ÇÇ', '„ÇÇ', '„ÇÇ„ÇÇ', '„ÅÆ', '„ÅÜ„Å°'], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("„Åì„Çì„Å´„Å°„ÅØ„ÄÇÁßÅ„ÅÆÂêçÂâç„ÅØ„Ç∑„É£„É≥„Åß„Åô„ÄÇ", LangId('ja'), tokenizer) == ["„Åì„Çì„Å´„Å°„ÅØ", "ÁßÅ", "„ÅÆ", "ÂêçÂâç", "„ÅØ", "„Ç∑„É£„É≥", "„Åß„Åô"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("„Åì„Çå„ÅØ„ÉÜ„Çπ„Éà„Åß„Åô„ÄÇ", LangId('ja'), tokenizer) == ["„Åì„Çå", "„ÅØ", "„ÉÜ„Çπ„Éà", "„Åß„Åô"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("ÁßÅ„ÅÆÂêçÂâç„ÅØÂ§™ÈÉé„Åß„Åô „ÄÇ", LangId('ja'), tokenizer) == ["ÁßÅ", "„ÅÆ", "ÂêçÂâç", "„ÅØ", "Â§™ÈÉé", "„Åß„Åô"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("„ÅÇ„ÅÆ„Ç§„É´„Ç´„ÅØ„Å®„Å¶„ÇÇ„Åã„Çè„ÅÑ„ÅÑÔºÅ", LangId('ja'), tokenizer) == ["„ÅÇ„ÅÆ", "„Ç§„É´„Ç´", "„ÅØ", "„Å®„Å¶„ÇÇ", "„Åã„Çè„ÅÑ„ÅÑ"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("Âêõ„ÅÆË®ÄËëâ„ÅØÊ∑±„ÅÑ„Åß„Åô„Å≠„ÄÇ", LangId('ja'), tokenizer) == ["Âêõ", "„ÅÆ", "Ë®ÄËëâ", "„ÅØ", "Ê∑±„ÅÑ", "„Åß„Åô", "„Å≠"], tokenizer.__name__


def test_get_word_tokens_from_text_user_tokenizer_zh():

    zh_tokenizers = [tokenizer for tokenizer in get_tokenizer_registry() if LangId('zh') in tokenizer.supported_languages()]

    assert(len(zh_tokenizers) >= 2)

    for tokenizer in zh_tokenizers:
        print("User tokenizer for ZH tested: "+tokenizer.name())
        assert TextProcessing.get_word_tokens_from_text("‰Ω†Ë∑üÊàë", LangId('zh'), tokenizer) == ['‰Ω†', 'Ë∑ü', 'Êàë'], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("hello ÂêÉÈ•≠‰∫ÜÂêóüòä? ", LangId('zh'), tokenizer) == ["ÂêÉÈ•≠", "‰∫Ü", "Âêó"], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text("ÊàëÁà±Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÇ", LangId('zh'), tokenizer) == ['Êàë', 'Áà±', 'Ëá™ÁÑ∂ËØ≠Ë®Ä', 'Â§ÑÁêÜ'], tokenizer.__name__
        assert TextProcessing.get_word_tokens_from_text(" Êàë Áà±Ëá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ„ÄÇ ", LangId('zh'), tokenizer) == ['Êàë', 'Áà±', 'Ëá™ÁÑ∂ËØ≠Ë®Ä', 'Â§ÑÁêÜ'], tokenizer.__name__


def test_create_word_token():

    create_word_token = TextProcessing.get_word_token_creator(LangId('en'))

    assert create_word_token("$$hellO") == "hello"
    assert create_word_token("hello!") == "hello"
    assert create_word_token("we‚Äôll") == "we'll"
    assert create_word_token("I√±t√´rn√¢ti√¥n√†liz√¶ti√∏n") == "i√±t√´rn√¢ti√¥n√†liz√¶ti√∏n"


def test_calc_word_presence_score():

    def calc_word_presence_score(context: Sequence[WordToken], token_index: int) -> tuple[WordToken, float]:
        return list(TextProcessing.calc_word_presence_scores(context))[token_index]

    token_context = TextProcessing.get_word_tokens_from_text("test", LangId("en"))
    assert calc_word_presence_score(token_context, 0) == (WordToken("test"), 1.0)

    token_context = TextProcessing.get_word_tokens_from_text("test abcd", LangId("en"))
    assert calc_word_presence_score(token_context, 0)[1] == (0.5+0.5+1)/3

    token_context = TextProcessing.get_word_tokens_from_text("test abcdabcd", LangId("en"))
    assert calc_word_presence_score(token_context, 0)[1] == (0.5+(1/3)+1)/3

    token_context = TextProcessing.get_word_tokens_from_text("test test abcd", LangId("en"))
    assert calc_word_presence_score(token_context, 1)[1] == ((2/3)+(2/3)+0.5)/3
