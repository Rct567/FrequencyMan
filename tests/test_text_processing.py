import re
from frequencyman.text_processing import USER_PROVIDED_TOKENIZERS_LOADED, LangId, TextProcessing


def test_acceptable_word():

    assert TextProcessing.acceptable_word("app-le") == True
    assert TextProcessing.acceptable_word("won't") == True
    assert TextProcessing.acceptable_word("iÃ±tÃ«rnÃ¢tiÃ´nÃ lizÃ¦tiÃ¸n") == True
    assert TextProcessing.acceptable_word("ä½ ", LangId("zh")) == True

    assert TextProcessing.acceptable_word("s") == False
    assert TextProcessing.acceptable_word("s ") == False
    assert TextProcessing.acceptable_word("s!!!") == False
    assert TextProcessing.acceptable_word("_s_") == False
    assert TextProcessing.acceptable_word("1234") == False
    assert TextProcessing.acceptable_word("$$$$$$$") == False


def test_get_plain_text():

    assert TextProcessing.get_plain_text("This a <b>sample</b></b> text with HTML<p>tags</P>and example.") == "This a sample text with HTML tags and example."
    assert TextProcessing.get_plain_text("This a <script> kk </script> example.") == "This a example."
    assert TextProcessing.get_plain_text("This N&amp;M that") == "This N&M that"
    assert TextProcessing.get_plain_text("Non-breaking space: This&nbsp;is&nbsp;a&nbsp;test.") == "Non-breaking space: This is a test."
    assert TextProcessing.get_plain_text("This [sound:azure-57d4ff9d-b02968eb-9f293f42-b1c6426f-2dd6b8a7.mp3]") == "This"
    assert TextProcessing.get_plain_text("Use of [brackets] <b>[inside HTML]</b> tags.") == "Use of [brackets] [inside HTML] tags."


def test_get_word_tokens_from_text():

    text_with_special_chars = "I can't \"believe\" it's here (wow) 1-character! 'amazinG'"
    assert TextProcessing.get_word_tokens_from_text(text_with_special_chars) == ["can't", "believe", "it's", "here", "wow", "1-character", "amazing"]

    # Test with an empty string
    assert TextProcessing.get_word_tokens_from_text("") == []
    assert TextProcessing.get_word_tokens_from_text("12345") == []

    assert TextProcessing.get_word_tokens_from_text("Simple sente-nce. EE.UU.") == ['simple', 'sente-nce', 'ee.uu']
    assert TextProcessing.get_word_tokens_from_text("This is a\ttest.\t") == ['this', 'is', 'test']
    assert TextProcessing.get_word_tokens_from_text("Hello, world!") == ['hello', 'world']

    assert TextProcessing.get_word_tokens_from_text("ĞœĞ½Ğµ â€¦ Ğ»ĞµÑ‚.") == ['Ğ¼Ğ½Ğµ', 'Ğ»ĞµÑ‚']
    assert TextProcessing.get_word_tokens_from_text("ä½ è·Ÿæˆ‘") == ['ä½ è·Ÿæˆ‘']


def test_get_word_tokens_from_text_user_tokenizer_ja():

    if not LangId('ja') in USER_PROVIDED_TOKENIZERS_LOADED:
        print("User tokenizer for JA test skipped!!!")
        return
    else:
        print("User tokenizer for JA tested: "+USER_PROVIDED_TOKENIZERS_LOADED[LangId('ja')].__name__)

    assert TextProcessing.get_word_tokens_from_text("ã™ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã‚‚ã®ã†ã¡", LangId('ja')) == ['ã™ã‚‚ã‚‚', 'ã‚‚', 'ã‚‚ã‚‚', 'ã‚‚', 'ã‚‚ã‚‚', 'ã®', 'ã†ã¡']
    assert TextProcessing.get_word_tokens_from_text("ã“ã‚“ã«ã¡ã¯ã€‚ç§ã®åå‰ã¯ã‚·ãƒ£ãƒ³ã§ã™ã€‚", LangId('ja')) == ["ã“ã‚“ã«ã¡ã¯", "ç§", "ã®", "åå‰", "ã¯", "ã‚·ãƒ£ãƒ³", "ã§ã™"]


def test_get_word_tokens_from_text_user_tokenizer_zh():

    if not LangId('zh') in USER_PROVIDED_TOKENIZERS_LOADED:
        print("User tokenizer for ZH test skipped!!!")
        return
    else:
        print("User tokenizer for ZH tested: "+USER_PROVIDED_TOKENIZERS_LOADED[LangId('zh')].__name__)

    assert TextProcessing.get_word_tokens_from_text("ä½ è·Ÿæˆ‘", LangId('zh')) == ['ä½ ', 'è·Ÿ', 'æˆ‘']
    assert TextProcessing.get_word_tokens_from_text("hello åƒé¥­äº†å—ğŸ˜Š? ", LangId('zh')) == ["åƒé¥­", "äº†", "å—"]
    assert TextProcessing.get_word_tokens_from_text("æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†ã€‚", LangId('zh')) == ['æˆ‘', 'çˆ±', 'è‡ªç„¶è¯­è¨€', 'å¤„ç†']


def test_create_word_token():

    assert TextProcessing.create_word_token("$$hellO") == "hello"
    assert TextProcessing.create_word_token("hello!") == "hello"
    assert TextProcessing.create_word_token("IÃ±tÃ«rnÃ¢tiÃ´nÃ lizÃ¦tiÃ¸n") == "iÃ±tÃ«rnÃ¢tiÃ´nÃ lizÃ¦tiÃ¸n"
